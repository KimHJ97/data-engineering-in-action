# Airflow 설치

## Airflow 기본 설치

```bash
$ sudo apt update
$ sudo apt install python3-pip

# Airflow 설치
$ pip install apache-airflow==2.5.2

# Airflow 실행
$ airflow
$ airflow standalone

# 만약 `airflow: command not found` 에러가 난다면?
# cd /usr/local/bin #airflow가 있는지 확인
# cd ~/.local/bin/ #airflow가 있는지 확인
# 기본 ubuntu에서 실행하는 명령어는 `/usr/local/bin`을 바라본다.

# 1안. 경로 이동
$ mv airflow /usr/local/bin/

# 2안. 환경변수 등록
$ export PATH=$PATH:~/.local/bin

# 계정 생성
$ airflow users  create --role Admin --username admin --email admin --firstname admin --lastname admin --password admin
```

 - `docker를 이용한 설치`
```bash
# Windows CMD
docker run -d ^
  -p 8080:8080 ^
  -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor ^
  -e AIRFLOW__CORE__FERNET_KEY=nZFtTrML26F2Vg5Rft8K3owULFyz1ddA7jWvQ7HL88o= ^
  -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db ^
  -v D:\practice\airflow\dags:/opt/airflow/dags ^
  -v D:\practice\airflow\logs:/opt/airflow/logs ^
  -v D:\practice\airflow\plugins:/opt/airflow/plugins ^
  apache/airflow:2.7.2 standalone

# Linux
docker run -d \
  -p 8080:8080 \
  -e AIRFLOW__CORE__EXECUTOR=SequentialExecutor \
  -e AIRFLOW__CORE__FERNET_KEY=$(python -c 'from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())') \
  -e AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////airflow/airflow.db \
  -v $(pwd)/dags:/opt/airflow/dags \
  -v $(pwd)/logs:/opt/airflow/logs \
  -v $(pwd)/plugins:/opt/airflow/plugins \
  apache/airflow:2.7.2 standalone

# 계정 생성
docker exec -it <container_id> airflow users create \
  --username admin \
  --firstname Admin \
  --lastname User \
  --role Admin \
  --email admin@example.com \
  --password admin

# 비밀번호 초기화
docker exec -it <container_id> airflow users update-password \
  --username admin \
  --password new_password
```

## airflog.cfg 파일

**airflow.cfg**는 Apache Airflow의 구성 파일로, Airflow 환경 및 동작 방식을 제어하는 다양한 설정을 포함하고 있습니다. 이 파일은 Airflow 설치 후 기본적으로 생성되며, 기본 경로는 $AIRFLOW_HOME/airflow.cfg입니다.

airflow.cfg 파일은 Airflow의 Scheduler, Web Server, Executor, Database 등과 같은 주요 구성 요소의 설정을 관리합니다.

```bash
# 파일 위치 확인
$ echo $AIRFLOW_HOME

# admin 계정 비밀번호
$ cat $AIRFLOW_HOME/standalone_admin_password.txt

# Airflow를 다시 시작하여 변경 사항 적용
$ airflow webserver -D
$ airflow scheduler -D
```

### 1. [core]

Airflow의 핵심 설정을 관리합니다.
 - dags_folder:
    - DAG 파일이 저장된 디렉토리 경로.
    - 기본값: $AIRFLOW_HOME/dags
 - executor:
    - 작업 실행 방법을 정의하는 Executor.
    - 주요 옵션:
        - SequentialExecutor: 단일 작업 순차 실행 (기본값).
        - LocalExecutor: 로컬에서 병렬 작업 실행.
        - CeleryExecutor: 분산 환경에서 작업 실행.
        - KubernetesExecutor: Kubernetes에서 작업 실행.
 - sql_alchemy_conn:
    - Airflow 메타데이터 데이터베이스의 연결 문자열.
    - 예: postgresql+psycopg2://user:password@localhost:5432/airflow
 - parallelism:
    - Airflow 전체에서 동시에 실행할 수 있는 작업 수.
 - load_examples:
    - True일 경우, 기본 예제 DAG를 로드.

### 2. [scheduler]

Scheduler의 동작 방식을 제어합니다.

 - scheduler_interval:
    - Scheduler가 DAG을 감지하고 작업을 배포하는 주기 (초 단위).
    - 기본값: 30
 - max_threads:
    - Scheduler가 동시에 처리할 수 있는 최대 스레드 수.
 - catchup_by_default:
    - True일 경우, 백필(backfill)을 활성화하여 과거 실행을 보충.
 - dag_dir_list_interval:
    - DAG 디렉토리를 스캔하는 주기 (초 단위).
 - parsing_processes
    - 스케줄러 프로세스 병렬 설정, 프로세스 수를 설정
    - DAG가 많을 경우 늘려줌
    - vCPU의 2배정도로 넣어줌

### 3. [webserver]

Web Server(Airflow UI)의 동작을 관리합니다.

 - web_server_port:
    - Web Server가 사용하는 포트.
    - 기본값: 8080
 - web_server_host:
    - Web Server가 바인딩될 호스트.
    - 기본값: 0.0.0.0 (모든 IP에서 접근 가능).
 - base_url:
    - Web Server의 기본 URL.
    - 예: http://localhost:8080
 - secret_key:
    - Flask Web Server에서 세션 암호화를 위한 비밀 키.
 - workers:
    - Gunicorn에서 사용할 Worker 수.

### 4. [database]

메타데이터 데이터베이스의 설정을 정의합니다.

 - sql_alchemy_pool_size:
    - SQLAlchemy 커넥션 풀의 크기.
 - sql_alchemy_max_overflow:
    - 최대 초과 연결 수.
 - load_default_connections:
    - 기본 연결 정보 로드 여부.

### 5. [celery]

CeleryExecutor와 관련된 설정을 정의합니다.

 - broker_url:
    - Celery 작업 큐의 브로커 URL.
    - 예: redis://localhost:6379/0 (Redis 사용 시).
 - result_backend:
    - 작업 결과를 저장할 백엔드.
    - 예: db+postgresql://user:password@localhost/airflow
 - worker_concurrency:
    - 각 Celery 워커가 동시에 처리할 수 있는 작업 수.
