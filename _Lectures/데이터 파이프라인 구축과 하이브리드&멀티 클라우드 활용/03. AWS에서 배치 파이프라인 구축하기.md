# AWS에서 배치 파이프라인 구축하기

## 1. AWS S3를 저장소로 하는 배치 파이프라인

### 1-1. AWS S3 소개

 - __Amazon Simple Storage Service(S3)__
    - 어디서나 원하는 양의 데이터를 저장하고 검색할 수 있도록 구축된 객체 스토리지
    - 업계 최고 수준의 내구성, 가용성, 성능, 보안 및 거의 무제한의 확장성을 아주 저렴한 요금으로 제공하는 단순한 스토리지 서비스
 - __AWS S3 특징__
    - 추가적인 시스템 관리 없이 많은 사용자가 데이터에 접근 가능
    - 논리적으로 저장되는 파일 수에 제한이 없으며, 하나의 파일 크기는 최대 5TB까지 저장 가능
    - 파일에 대한 접근 제한 기능 제공
    - 데이터 손실을 걱정할 필요 없음
    - 버전 관리를 통해 복원이 가능
    - 데이터의 중요도에 따라 스토리지 클래스별로 구분 저장 가능하며 이를 통해 비용 절감 가능
 - __S3 관련 용어__
    - 객체(Object): S3에 저장된 하나의 데이터(파일, 폴더 등)
    - 버킷(Bucket): 객체들을 보관하는 최상위 디렉토리, 버킷 단위로 다양한 보안 설정이 가능하며 Region 별로 생성
    - 버전 관리: 버전 관리는 객체의 여러 버전을 동일한 버킷에서 관리하기 위한 수단, 각 버전을 보존, 검색 및 복원 가능
    - 스토리지 클래스: 데이터 액세스, 복원력 및 비용 요구 사항에 따라 선택할 수 있는 스토리지 클래스 제공. 가장 기본이 되는 S3 Standard, 자주 액세스 하지 않는 S3 Standard-IA, 거의 액세스 하지 않는 장기 데이터 S3 Glacier Flexible Retrieval 등
 - __AWS S3 버킷 생성 및 데이터 저장__
    - Amazon S3 > 버킷 > 버킷 만들기
        - 버킷 이름: 버킷의 이름
        - AWS 리전: 버킷의 리전. 버킷은 AWS 리전 별로 존재한다.
        - 객체 소유권: ACL 설정(버킷에 저장된 오브젝트의 소유권 및 액세스 제어 목록을 제어)
        - 퍼블릭 액세스 차단 설정
        - 버킷 버전 관리: 의도치 않게 삭제될 경우 버전 관리가 활성화되었다면 복구가 가능
        - 태그: 태그를 지정하여 스토리지 별로 비용을 추적할 수 있다.
```bash
$ aws configure
AWS Access  Key ID
AWS Secret Access Key
Default region name [ap-northeast-2]
Default output format [json]

# 생성된 버킷 확인
$ aws s3 ls /

# 특정 버킷 내에 파일 확인
$ aws s3 ls s3://{butket-name}/

# 실습에 활용할 데이터셋(aws public dataset) 확인
$ aws s3 ls s3://nyc-tlc/
$ aws s3 ls s3://nyc-tlc/csv_backup/
$ aws s3 ls s3://nyc-tlc/"trip data"/
$ aws s3 ls s3://nyc-tlc/"trip data"/ | grep 2022-10

# 데이터셋 복사
$ aws s3 cp s3://nyc-tlc/"trip data"/green_tripdata_2022-10.parquet s3://{butket-name}/input/green_tripdata_2022-10.parquet
$ aws s3 cp s3://nyc-tlc/"trip data"/yellow_tripdata_2022-10.parquet s3://{butket-name}/input/yellow_tripdata_2022-10.parquet

$ aws s3 ls s3://{butket-name}/input/
```

### 1-2. Apache Spark

 - 빅데이터 워크로드에 쓰이는 오픈소스 고속 통합 분석 엔진
 - 2009년 UC Berkeley에서 개발됨.
 - 데이터 처리 분야에서 가장 규모가 큰 오픈소스 프로젝트 중 하나.
 - Netflix, Yahoo, eBay와 같은 대형 인터넷 기업들이 대규모로 Spark를 사용하며, 8,000개 이상의 클러스터에서 페타바이트(PB) 규모의 데이터를 처리함.
 - 현재 250개 이상의 조직에서 1,000명 이상이 기여 중.
 - 인메모리 기반 데이터 처리로, 하둡 맵리듀스(Hadoop MapReduce) 대비 최대 100배 빠른 처리 속도(디스크 사용 시 약 10배 빠름).
 - DAG 스케줄러, 쿼리 최적화 도구, 물리적 실행 엔진을 활용하여 고성능을 제공.
 - 다양한 데이터 저장소 및 생태계와 잘 통합됨.
 - 배치 및 실시간 처리뿐만 아니라 머신러닝 애플리케이션(MLlib)과 그래프 처리(GraphX)를 지원.

 - __Spark의 장점__
    - 속도
        - 여러 병렬 작업에 걸쳐 데이터를 메모리에 캐시하여 빠른 실행 속도를 자랑합니다.
        - 하둡 MapReduce 대비 최대 100배 빠르고, 디스크 기반 처리 시 10배 빠름.
        - 디스크 내 대규모 정렬에서 세계 신기록을 보유하고 있습니다.
    - 실시간 스트림 처리
        - 실시간 스트리밍(mini-batch) 데이터 처리가 가능하며, 다른 프레임워크와의 통합도 지원합니다.
    - 통합 엔진 (여러 워크로드 지원)
        - SQL 쿼리, 스트리밍 데이터, 머신러닝, 그래프 처리를 지원합니다.
        - 높은 수준의 라이브러리 패키지를 통해 생산성을 향상시키고 복잡한 워크플로를 구현할 수 있습니다.
    - 사용 편리성 증가
        - Java, Scala, Python, R과 같은 다양한 프로그래밍 언어를 지원합니다.
        - 데이터 변환을 위한 100개 이상의 연산자 컬렉션을 제공하며, 반구조화된 데이터 조작에 유용한 DataFrame API를 지원합니다.
 - __Apache Spark 에코 시스템__
    - Spark Core API (일반 실행): Spark Core는 Spark 플랫폼의 기본 일반 실행 엔진입니다.
    - Spark SQL + DataFrames (구조화된 데이터): Spark SQL은 구조화된 데이터 처리를 위한 Spark 모듈입니다.
    - 스트리밍 (스트리밍 분석): Spark의 사용 편의성과 내고장성을 그대로 활용하면서도, 스트리밍 데이터와 과거 데이터를 결합하여 강력한 인터랙티브 분석 애플리케이션을 지원합니다.
    - MLlib (머신러닝): MLlib은 확장 가능한 머신러닝 라이브러리로, 고급 알고리즘과 빠른 속도를 제공합니다.
    - GraphX (그래프 계산): GraphX는 Spark를 기반으로 한 그래프 계산 엔진입니다. 사용자가 대규모의 구조화된 그래프 데이터를 상호작용 방식으로 구축, 변환, 추론할 수 있도록 지원합니다.


